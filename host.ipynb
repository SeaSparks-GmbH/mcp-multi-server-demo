{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Imports",
   "id": "70bcf545f064c8eb"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Start a Server",
   "id": "6d92675a482e686d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import subprocess\n",
    "\n",
    "# Global list to keep track of all subprocesses\n",
    "notebook_subprocesses = []"
   ],
   "id": "406924d06314ea6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Math Server (port 58000)\n",
    "proc = subprocess.Popen([\"python\", \"math_server.py\"])\n",
    "notebook_subprocesses.append(proc)"
   ],
   "id": "45f3b5975dbe0c5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Start another server",
   "id": "5c942391c85fd163"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Weather Server (port 58001)\n",
    "proc = subprocess.Popen([\"python\", \"weather_server.py\"])\n",
    "notebook_subprocesses.append(proc)"
   ],
   "id": "16f912f8bd219458",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# The Client",
   "id": "cbe1d4e129125b00"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "from client import MultiServerClient",
   "id": "511fb73b2ca9148d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# The host",
   "id": "35a225050902d673"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import json\n",
    "from openai import AsyncOpenAI\n",
    "\n",
    "openai_key = \"your-api-key-here\"  # Replace with your key\n",
    "\n",
    "# Method to query the llm\n",
    "async def query_openai(prompt):\n",
    "    llm_client = AsyncOpenAI(api_key=oai_key)\n",
    "    return await llm_client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "\n",
    "async def run_host_query(user_input: str):\n",
    "\n",
    "    endpoints = {\n",
    "        \"math_server\": \"http://127.0.0.1:58000/sse\",\n",
    "        \"weather_server\": \"http://127.0.0.1:58001/sse\"\n",
    "    }\n",
    "\n",
    "    # Initialize connections to all servers\n",
    "    mcp_client = MultiServerClient(endpoints)\n",
    "    print(\"Connect to servers...\")\n",
    "    await mcp_client.connect_all()\n",
    "\n",
    "    # Get all tools from all servers\n",
    "    tools_by_server = await mcp_client.list_all_tools()\n",
    "    tool_summary = []\n",
    "    for server, tools in tools_by_server.items():\n",
    "        for tool in tools:\n",
    "            tool_summary.append(f\"server: {server}, tool: {tool.name}, description: {tool.description} input schema: {tool.inputSchema}\")\n",
    "    print(\"Tool summaries:\", tool_summary)\n",
    "\n",
    "    # Define a simple prompt which tells the llm which tools are available and how to structure the response\n",
    "    prompt = f\"\"\"\n",
    "You are a tool routing assistant. Choose the best tool for the user query. Available tools:\n",
    "\n",
    "{chr(10).join(tool_summary)}\n",
    "\n",
    "Given the user query: \"{user_input}\"\n",
    "Respond only with a JSON object like:\n",
    "{{\"server\": \"math_server\", \"tool\": \"add\", \"args\": {{\"a\": 3, \"b\": 5}}}}\n",
    "\"\"\"\n",
    "\n",
    "    # Query the llm\n",
    "    response = await query_openai(prompt)\n",
    "\n",
    "    # Extract the llm response content\n",
    "    raw_content = response.choices[0].message.content\n",
    "\n",
    "    # Clean up markdown if present\n",
    "    cleaned_content = raw_content.strip().strip(\"```json\").strip(\"```\").strip()\n",
    "\n",
    "    # Parse JSON to see which tool was chosen\n",
    "    parsed = json.loads(cleaned_content)\n",
    "    print(\"LLM chose:\", parsed)\n",
    "\n",
    "    # Call the tool via the MCP client\n",
    "    result = await mcp_client.call(parsed[\"server\"], parsed[\"tool\"], parsed[\"args\"])\n",
    "    print(\"Tool result:\", result)\n",
    "\n",
    "    # Disconnect all\n",
    "    await mcp_client.disconnect_all()"
   ],
   "id": "f09d7fc8a02f4ed3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "await run_host_query(\"What's the weather like in Berlin?\")",
   "id": "645bebb8c89266f4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "await run_host_query(\"How many hours do 5 days have?\")",
   "id": "bafdbe9fd1fad3c0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Stop Servers",
   "id": "6d4dacbd5c44dac4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for proc in notebook_subprocesses:\n",
    "    proc.terminate()  # or proc.kill() for force\n",
    "notebook_subprocesses.clear()"
   ],
   "id": "d4a5d3ccaf6c70b4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "2c8fab8cc6f4b3f8",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mcp_test] *",
   "language": "python",
   "name": "conda-env-mcp_test-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
